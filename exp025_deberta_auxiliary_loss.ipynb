{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"exp025\"\n",
    "\n",
    "path = Path(f\"../outputs/{exp}\")\n",
    "path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "clothing_master_df = pd.read_csv(\"../data/clothing_master.csv\")\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_column_names = {\n",
    "    \"Clothing ID\": \"clothing_id\",\n",
    "    \"Age\": \"age\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Review Text\": \"review_text\",\n",
    "    \"Rating\": \"rating\",\n",
    "    \"Recommended IND\": \"recommended\",\n",
    "    \"Positive Feedback Count\": \"positive_feedback_count\",\n",
    "}\n",
    "\n",
    "test_column_names = {\n",
    "    \"Clothing ID\": \"clothing_id\",\n",
    "    \"Age\": \"age\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Review Text\": \"review_text\",\n",
    "}\n",
    "\n",
    "clothing_master_column_names = {\n",
    "    \"Clothing ID\": \"clothing_id\",\n",
    "    \"Division Name\": \"division_name\",\n",
    "    \"Department Name\": \"department_name\",\n",
    "    \"Class Name\": \"class_name\",\n",
    "}\n",
    "\n",
    "train_df = train_df.rename(columns=train_column_names)\n",
    "test_df = test_df.rename(columns=test_column_names)\n",
    "clothing_master_df = clothing_master_df.rename(columns=clothing_master_column_names)\n",
    "\n",
    "train_df = pd.merge(train_df, clothing_master_df, on=\"clothing_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, clothing_master_df, on=\"clothing_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512, mode=\"train\"):\n",
    "        self.texts = df[\"text\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.labels = df[\"recommended\"]\n",
    "            self.clothing_ids = df[\"clothing_id\"]\n",
    "            self.rating = df[\"rating\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            label = self.labels[idx]\n",
    "            output = {\n",
    "                \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "                \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "                \"rating\": torch.tensor(self.rating[idx], dtype=torch.float32),\n",
    "            }\n",
    "            return output\n",
    "        else:\n",
    "            output = {\n",
    "                \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            }\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        return (x * mask.unsqueeze(-1)).sum(1) / mask.sum(1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class BERTModel(torch.nn.Module):\n",
    "    def __init__(self, model_name: str, num_labels: int, tokenizer=None):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.pooling = MeanPooling()\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.rating_head = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "        self.freeze(4)\n",
    "\n",
    "    def freeze(self, num_freeze_layers):\n",
    "        \"\"\"\n",
    "        Freeze the BERT model up to num_freeze_layers\n",
    "        \"\"\"\n",
    "        for layer in self.bert.encoder.layer[:num_freeze_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels=None,\n",
    "        rating=None,\n",
    "    ):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.pooling(outputs.last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            rating_pred = self.rating_head(output).squeeze(dim=-1)\n",
    "            ce_loss = self.loss_fn(logits, labels)\n",
    "            rating_loss = self.mse(rating_pred, rating)\n",
    "\n",
    "            loss = ce_loss + rating_loss\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_column(df: pd.DataFrame, sep_token: str) -> pd.DataFrame:\n",
    "    text_df = df.copy()\n",
    "    text_df[\"text\"] = (\n",
    "        \"title: \"\n",
    "        + text_df[\"title\"].fillna(\"no title\").astype(str)\n",
    "        + sep_token\n",
    "        + \"review_text: \"\n",
    "        + text_df[\"review_text\"].fillna(\"no review\").astype(str)\n",
    "        + sep_token\n",
    "        + \"age: \"\n",
    "        + text_df[\"age\"].fillna(\"nan\").astype(str)\n",
    "        + sep_token\n",
    "        + \"division_name: \"\n",
    "        + text_df[\"division_name\"].fillna(\"nan\").astype(str)\n",
    "        + sep_token\n",
    "        + \"department_name: \"\n",
    "        + text_df[\"department_name\"].fillna(\"nan\").astype(str)\n",
    "        + sep_token\n",
    "        + \"class_name: \"\n",
    "        + text_df[\"class_name\"].fillna(\"nan\").astype(str)\n",
    "    )\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = torch.tensor(pred.predictions)\n",
    "    preds = F.softmax(preds, dim=1)[:, 1].numpy()\n",
    "\n",
    "    if len(set(labels)) > 1:\n",
    "        auc = roc_auc_score(labels, preds)\n",
    "        return {\"auc\": auc}\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add newline Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "new_tokens = [\"<newline>\"]\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_newlines_with_token(text, token=\"<newline>\"):\n",
    "    return text.replace(\"\\n\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df[\"recommended\"].to_numpy()\n",
    "\n",
    "train_text_df = create_text_column(train_df, sep_token=tokenizer.sep_token)\n",
    "test_text_df = create_text_column(test_df, sep_token=tokenizer.sep_token)\n",
    "\n",
    "train_text_df[\"text\"] = train_text_df[\"text\"].apply(replace_newlines_with_token)\n",
    "test_text_df[\"text\"] = test_text_df[\"text\"].apply(replace_newlines_with_token)\n",
    "\n",
    "test_dataset = TextDataset(test_text_df, tokenizer, max_len=192, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:848: UserWarning: The groups parameter is ignored by StratifiedKFold\n",
      "  warnings.warn(\n",
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshu421\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shugo/kaggle/competitions/atmacup17/notebooks/wandb/run-20240831_155353-rg13562z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shu421/huggingface/runs/rg13562z' target=\"_blank\">../outputs/exp025</a></strong> to <a href='https://wandb.ai/shu421/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shu421/huggingface' target=\"_blank\">https://wandb.ai/shu421/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shu421/huggingface/runs/rg13562z' target=\"_blank\">https://wandb.ai/shu421/huggingface/runs/rg13562z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 16:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.346000</td>\n",
       "      <td>0.862409</td>\n",
       "      <td>0.949706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.787335</td>\n",
       "      <td>0.963363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.606786</td>\n",
       "      <td>0.963181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.631900</td>\n",
       "      <td>0.588965</td>\n",
       "      <td>0.966034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.590500</td>\n",
       "      <td>0.645538</td>\n",
       "      <td>0.973004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.395500</td>\n",
       "      <td>0.713999</td>\n",
       "      <td>0.970397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>0.600110</td>\n",
       "      <td>0.971741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.539679</td>\n",
       "      <td>0.974131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.525679</td>\n",
       "      <td>0.973574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.504025</td>\n",
       "      <td>0.974868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.540370</td>\n",
       "      <td>0.974873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.527392</td>\n",
       "      <td>0.975092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.521184</td>\n",
       "      <td>0.975332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.534672</td>\n",
       "      <td>0.975340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.453200</td>\n",
       "      <td>0.531062</td>\n",
       "      <td>0.975324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 16:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.788369</td>\n",
       "      <td>0.942808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>1.058834</td>\n",
       "      <td>0.969434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.729900</td>\n",
       "      <td>0.596365</td>\n",
       "      <td>0.970071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>0.572534</td>\n",
       "      <td>0.969119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.596622</td>\n",
       "      <td>0.971782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.585214</td>\n",
       "      <td>0.969578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.550899</td>\n",
       "      <td>0.974594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.501674</td>\n",
       "      <td>0.973937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.514106</td>\n",
       "      <td>0.972905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.578184</td>\n",
       "      <td>0.972568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.545310</td>\n",
       "      <td>0.972988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.508312</td>\n",
       "      <td>0.972783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.525417</td>\n",
       "      <td>0.973082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.526371</td>\n",
       "      <td>0.972940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.529433</td>\n",
       "      <td>0.972909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 16:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.830400</td>\n",
       "      <td>0.947290</td>\n",
       "      <td>0.930074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>0.614818</td>\n",
       "      <td>0.960961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>0.622003</td>\n",
       "      <td>0.967142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.553373</td>\n",
       "      <td>0.969574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>0.583163</td>\n",
       "      <td>0.972440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.521775</td>\n",
       "      <td>0.975846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.565306</td>\n",
       "      <td>0.971364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.425100</td>\n",
       "      <td>0.583154</td>\n",
       "      <td>0.972972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.599060</td>\n",
       "      <td>0.973822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.567931</td>\n",
       "      <td>0.973707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>0.529158</td>\n",
       "      <td>0.974895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.503926</td>\n",
       "      <td>0.975538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.506803</td>\n",
       "      <td>0.975372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.516588</td>\n",
       "      <td>0.975119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.418100</td>\n",
       "      <td>0.516161</td>\n",
       "      <td>0.975207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 16:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>1.414712</td>\n",
       "      <td>0.913845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.695500</td>\n",
       "      <td>0.721839</td>\n",
       "      <td>0.946867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.663318</td>\n",
       "      <td>0.958243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.631800</td>\n",
       "      <td>0.576784</td>\n",
       "      <td>0.962774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.528800</td>\n",
       "      <td>0.594078</td>\n",
       "      <td>0.965556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.364100</td>\n",
       "      <td>0.656586</td>\n",
       "      <td>0.966049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.356800</td>\n",
       "      <td>0.688003</td>\n",
       "      <td>0.963138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.300200</td>\n",
       "      <td>0.531404</td>\n",
       "      <td>0.966284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.553700</td>\n",
       "      <td>0.512427</td>\n",
       "      <td>0.968852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>0.522218</td>\n",
       "      <td>0.969206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.545552</td>\n",
       "      <td>0.968519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.551436</td>\n",
       "      <td>0.969703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.549074</td>\n",
       "      <td>0.969881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.541869</td>\n",
       "      <td>0.969987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.291600</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>0.969955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shugo/kaggle/competitions/atmacup17/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 16:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.914464</td>\n",
       "      <td>0.904659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.654985</td>\n",
       "      <td>0.965749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.812767</td>\n",
       "      <td>0.968573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.517300</td>\n",
       "      <td>0.548471</td>\n",
       "      <td>0.969716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.566300</td>\n",
       "      <td>0.508103</td>\n",
       "      <td>0.969769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.590014</td>\n",
       "      <td>0.969637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.570586</td>\n",
       "      <td>0.971595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.559180</td>\n",
       "      <td>0.972527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.532066</td>\n",
       "      <td>0.970782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.534174</td>\n",
       "      <td>0.971262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.511810</td>\n",
       "      <td>0.970766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.547700</td>\n",
       "      <td>0.539729</td>\n",
       "      <td>0.970783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.395500</td>\n",
       "      <td>0.527965</td>\n",
       "      <td>0.970789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.522778</td>\n",
       "      <td>0.970882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.565400</td>\n",
       "      <td>0.523611</td>\n",
       "      <td>0.970913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.9725615635677455\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(train_df))\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    cv.split(train_df, labels, groups=train_df[\"clothing_id\"])\n",
    "):\n",
    "    _train_text_df = train_text_df.iloc[train_idx].reset_index(drop=True)\n",
    "    _valid_text_df = train_text_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TextDataset(\n",
    "        _train_text_df,\n",
    "        tokenizer,\n",
    "        max_len=192,\n",
    "    )\n",
    "    val_dataset = TextDataset(_valid_text_df, tokenizer, max_len=192)\n",
    "\n",
    "    model = BERTModel(model_name, num_labels=2, tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"../outputs/{exp}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=1e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"../outputs/{exp}/logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"auc\",\n",
    "        greater_is_better=True,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        dataloader_num_workers=4,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(f\"../outputs/{exp}/fold{fold}\")\n",
    "\n",
    "    valid_pred = trainer.predict(val_dataset)\n",
    "    valid_pred = torch.tensor(valid_pred.predictions)\n",
    "    valid_pred = F.softmax(valid_pred, dim=1)[:, 1].numpy()\n",
    "    oof[val_idx] = valid_pred\n",
    "\n",
    "    np.save(f\"../outputs/{exp}/valid_pred_fold{fold}.npy\", valid_pred)\n",
    "\n",
    "np.save(f\"../outputs/{exp}/oof.npy\", oof)\n",
    "overall_auc = roc_auc_score(labels, oof)\n",
    "print(f\"Overall AUC: {overall_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99894567 0.47369413 0.99834218 ... 0.99875703 0.99887633 0.99915528]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def load_model_and_predict(model_path, test_dataset, model_class, model_name):\n",
    "    model = model_class(model_name, num_labels=2, tokenizer=tokenizer)\n",
    "    state_dict = load_file(f\"{model_path}/model.safetensors\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./outputs\",\n",
    "        per_device_eval_batch_size=16,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    predictions = torch.tensor(predictions.predictions)\n",
    "    predictions = F.softmax(predictions, dim=1)[:, 1].numpy()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "test_text_df = create_text_column(test_df, sep_token=tokenizer.sep_token)\n",
    "test_dataset = TextDataset(test_text_df, tokenizer, max_len=256, mode=\"test\")\n",
    "\n",
    "test_predictions = np.zeros((len(test_text_df), 5))\n",
    "\n",
    "for fold in range(5):\n",
    "    model_path = f\"../outputs/{exp}/fold{fold}\"\n",
    "    test_predictions[:, fold] = load_model_and_predict(\n",
    "        model_path, test_dataset, BERTModel, model_name\n",
    "    )\n",
    "\n",
    "np.save(f\"../outputs/{exp}/test_predictions_{exp}.npy\", test_predictions)\n",
    "final_predictions = test_predictions.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.473694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11150</th>\n",
       "      <td>0.998645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11151</th>\n",
       "      <td>0.998808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11152</th>\n",
       "      <td>0.998757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11153</th>\n",
       "      <td>0.998876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11154</th>\n",
       "      <td>0.999155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11155 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target\n",
       "0      0.998946\n",
       "1      0.473694\n",
       "2      0.998342\n",
       "3      0.037251\n",
       "4      0.998061\n",
       "...         ...\n",
       "11150  0.998645\n",
       "11151  0.998808\n",
       "11152  0.998757\n",
       "11153  0.998876\n",
       "11154  0.999155\n",
       "\n",
       "[11155 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_df[\"target\"] = final_predictions\n",
    "sample_submission_df.to_csv(f\"../outputs/{exp}/submission_{exp}.csv\", index=False)\n",
    "sample_submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
